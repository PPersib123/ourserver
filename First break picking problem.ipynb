{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First break picking problem\n",
    "\n",
    "First-break picking is the task of determining, given a set of raw seismic traces, the onsets of the first signal arrivals as accurately as possible. The accurate determination of the first arrivals onset first-break times is needed for calculating the static corrections, a fundamental stage of seismic data processing.\n",
    "\n",
    "    Datasets\n",
    "    Model architecture\n",
    "    Training\n",
    "    Inference\n",
    "    Model evaluation\n",
    "    Running time\n",
    "    Criticism\n",
    "    Summary\n",
    "    Suggestions for improvement\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('./SeismicPro/')\n",
    "\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from tensorflow import logging\n",
    "logging.set_verbosity(logging.ERROR)\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "from seismicpro.batchflow import B, V, W\n",
    "from seismicpro.batchflow.models.torch import ResNet34, TorchModel\n",
    "from seismicpro.src import SeismicDataset, FieldIndex, TraceIndex, seismic_plot\n",
    "from seismicpro.models.metrics import PickingMetrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Datasets\n",
    "\n",
    "We will train a model using raw traces from one of the company's anonymized surveys:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_train = '/data/day1/data1.sgy'\n",
    "markup_path='/data/day1/data1.csv'\n",
    "index = FieldIndex(name='raw', path=path_train, markup_path=markup_path)\n",
    "\n",
    "index.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FIRST_BREAK_TIME \tTraceNumber \tTRACE_SEQUENCE_FILE \tfile_id\n",
    "\t\t\traw \traw\n",
    "FieldRecord \t\t\t\t\n",
    "16005 \t74.997650 \t2268 \t1677 \t/data/day1/data1.sgy\n",
    "16005 \t72.829834 \t2269 \t1678 \t/data/day1/data1.sgy\n",
    "16005 \t114.031898 \t2267 \t1679 \t/data/day1/data1.sgy\n",
    "16005 \t103.484001 \t2270 \t1680 \t/data/day1/data1.sgy\n",
    "16005 \t136.032379 \t2266 \t1681 \t/data/day1/data1.sgy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load one seismogram and coresponding picking.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = (SeismicDataset(index).next_batch(batch_size=10)\n",
    "\n",
    "        .load(components='raw', fmt='segy')\n",
    "\n",
    "        .load(components='markup', fmt='picks'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Draw the whole seismogram and 10 zoomed traces with labeled picking too see the pattern of picking.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = 2000\n",
    "\n",
    "(batch.seismic_plot('raw', index.indices[0], src_picking='markup', cmap='gray',\n",
    "\n",
    "                     figsize=(15,5), vmax=cv, vmin=-cv, s=5, scatter_color='r')\n",
    "\n",
    "      .seismic_plot('raw', index.indices[0], src_picking='markup', cmap='gray',\n",
    "\n",
    "                    figsize=(15,5), vmax=cv, vmin=-cv, s=70, scatter_color='r',\n",
    "\n",
    "                    wiggle=True, xlim=(30,40), ylim=(100,200), std=0.1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can split data to train and test parts:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index.split(0.8, shuffle=42)\n",
    "\n",
    "train_data = SeismicDataset(TraceIndex(index.train))\n",
    "\n",
    "test_data = SeismicDataset(TraceIndex(index.test))\n",
    "\n",
    "print('Traces in train: {}'.format(len(train_data)))\n",
    "\n",
    "print('Traces in test: {}'.format(len(test_data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model architecture\n",
    "\n",
    "We are using simple VGG-like CNN with encoder 4 blocks and head.\n",
    "\n",
    "Model's configuration:\n",
    "\n",
    "    encoder block: Conv1d - downsample - Relu\n",
    "    head: global pooling - dropout - fully-connected\n",
    "\n",
    "Layes parameters:\n",
    "\n",
    "Conv1d(body):\n",
    "\n",
    "    filters = [8, 16, 32, 64]\n",
    "    kernel_size = 3\n",
    "    padding = 'same'\n",
    "\n",
    "Dropout:\n",
    "\n",
    "    dropout_rate = 0.1\n",
    "\n",
    "Fully-connected:\n",
    "\n",
    "    units = 1\n",
    "\n",
    "Activation:\n",
    "\n",
    "    activation = Relu\n",
    "\n",
    "Here is a config for regression model:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_config = {\n",
    "    'raw': {'shape': (1, W(B('raw')).shape[2])}, \n",
    "    'targets': {'shape': (1,)}\n",
    "    }\n",
    "\n",
    "config = {\n",
    "    'inputs': inputs_config,\n",
    "    'initial_block/inputs': 'raw',\n",
    "    'body': dict(layout='cpa'*4, filters=[8, 16, 32, 64], kernel_size=3),\n",
    "    'head': dict(layout='Vdf', dropout_rate=.1, units=1),\n",
    "    'optimizer': dict(name='SGD', lr=0.01),\n",
    "    'loss': 'l1',\n",
    "    'device': 'gpu:0',\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training\n",
    "\n",
    "Definition of batch size and number of training iterations:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "\n",
    "N_ITERS = 4000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training pipeline actions:\n",
    "\n",
    "    Initialize model\n",
    "    Load raw traces and labels.\n",
    "    Normalize the traces to the zero mean and unit variance.\n",
    "    Preprocess the batch of traces to make it compatible with torch models.\n",
    "    Perform training step\n",
    "\n",
    "Set the train pipeline.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pipeline = (train_data.p\n",
    "                      .init_model('dynamic', TorchModel, 'my_model', config=config)\n",
    "                      .init_variable('loss', [])\n",
    "                      .load(components='raw', fmt='segy')\n",
    "                      .load(components='markup', fmt='picks')\n",
    "                      .standardize(src='raw', dst='raw')\n",
    "                      .apply_transform_all(src='raw', dst='raw', func=lambda x: np.stack(x))\n",
    "                      .apply_transform_all(src='markup', dst='markup', func=lambda x: np.stack(x).astype(np.float32))\n",
    "                      .train_model('my_model', B('raw'), B('markup'), fetches='loss', save_to=V('loss', mode='a'))\n",
    "                      .run_later(BATCH_SIZE, n_iters=N_ITERS, drop_last=True, shuffle=True, bar=True)\n",
    "                 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is also possible to use n_iters parameter instead of n_epochs to specify how many batches you want to feed to the model.\n",
    "\n",
    "Run the train pipeline.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pipeline.run(bar_desc=W(V('loss')[-1].format('Loss is: {:7.7}')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loss function plot:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = train_pipeline.get_variable('loss')\n",
    "\n",
    "plt.figure(figsize=(15,8))\n",
    "\n",
    "plt.grid(True)\n",
    "\n",
    "plt.xlabel(\"Iterations\"), plt.ylabel(\"Loss\")\n",
    "\n",
    "plt.plot(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Inference\n",
    "\n",
    "Inference pipeline is similar to the training pipeline:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pipeline = (test_data.p\n",
    "                      .import_model('my_model', train_pipeline) \n",
    "                      .init_variable('targets', [])\n",
    "                      .init_variable('traces', [])\n",
    "                      .init_variable('predictions', [])\n",
    "                      .load(components='raw', fmt='segy')\n",
    "                      .load(components='markup', fmt='picks')\n",
    "                      .add_components(components='predictions')\n",
    "                      .standardize(src='raw', dst='raw')\n",
    "                      .apply_transform_all(src='raw', dst='raw', func=lambda x: np.stack(x))\n",
    "                      .apply_transform_all(src='markup', dst='markup', func=lambda x: np.stack(x))\n",
    "                      .predict_model('my_model', B('raw'), fetches='predictions', save_to=B('predictions', mode='a'))\n",
    "                      .update(V('traces', 'a'), B('raw'))\n",
    "                      .update(V('targets', 'a'), B('markup'))\n",
    "                      .update(V('predictions', 'a'), B('predictions'))\n",
    "                      .run_later(2000, n_epochs=1, drop_last=False, shuffle=False, bar=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Run the inference pipeline on test part of train data:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pipeline.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Postprocessing results by concatinating all traces and pickings:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = np.concatenate(test_pipeline.get_variable('predictions'))\n",
    "\n",
    "targets = np.concatenate(test_pipeline.get_variable('targets'))\n",
    "\n",
    "traces = np.squeeze(np.concatenate(test_pipeline.get_variable('traces')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Preds shape: {}'.format(preds.shape))\n",
    "print('Targets shape: {}'.format(targets.shape))\n",
    "print('Traces shape: {}'.format(traces.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Model evaluation\n",
    "\n",
    "Creating an object to calculate metrics:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = PickingMetrics(targets, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Model evaluation\n",
    "\n",
    "Creating an object to calculate metrics:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = PickingMetrics(targets, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('MAE on test: {0:.3f}'.format(metrics.evaluate('mae')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "plt.title('Absolute error distribution')\n",
    "plt.xlabel('Error, samples')\n",
    "plt.ylabel('Number of traces')\n",
    "_ = plt.hist(abs(targets - preds), range=(0,200), bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Visual estimation\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset = slice(0,400)\n",
    "\n",
    "pts_pred = (range(len(preds[subset])), preds[subset]/2)\n",
    "\n",
    "pts_true = (range(len(targets[subset])), targets[subset]/2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Draw the seismogram with predictions (blue) and targets (red).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = 1\n",
    "\n",
    "seismic_plot(traces[subset], cmap='gray', vmax=cv, vmin=-cv, pts=pts_pred,\n",
    "\n",
    "             s=20, scatter_color='b', figsize=(15, 5), names=['Predictions'])\n",
    "\n",
    "seismic_plot(traces[subset], cmap='gray', vmax=cv, vmin=-cv, pts=pts_true,\n",
    "\n",
    "             s=20, scatter_color='r', figsize=(15, 5), names=['Targets'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running time\n",
    "\n",
    "System config:\n",
    "\n",
    "    GPU: GTX GeForce 2080ti\n",
    "    CPU: Intel Xeon E5-2630\n",
    "\n",
    "Time performance:\n",
    "\n",
    "    Model training iteration with batch size = 64: 0.05 sec.\n",
    "    Inference iteration with batch size = 1000: 0.12 sec.\n",
    "\n",
    "Criticism\n",
    "Summary\n",
    "Suggestions for improvement\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
